{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning 1B LLaMA 3.2: A Comprehensive Step-by-Step Guide with Code\n",
    "\n",
    "Source: \n",
    "\n",
    "https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article\n",
    "\n",
    "Resources used:\n",
    "\n",
    "- **Unsloth** enhances the efficiency of fine-tuning large language models (LLMs) specially LLaMA and Mistral. \n",
    " \n",
    "- With Unsloth, we can use advanced quantization techniques, such as 4-bit and 16-bit quantization, to reduce the memory and speed up both training and inference.\n",
    "\n",
    "- Unsloth broad compatibility and customization options allow to do the quantization process to fit the specific needs of products. \n",
    "\n",
    "- This flexibility combined with its ability to cut VRAM usage by up to 60%.\n",
    "\n",
    "https://docs.unsloth.ai/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy pandas matplotlib seaborn scikit-learn jupyter notebook ipykernel \n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Mental Health Chatbot by fine tuning Llama 3.2\n",
    "\n",
    "Mental health is a critical aspect of overall well being for emotional, psychological and social dimensions.\n",
    "\n",
    "We are going to fine-tune the LLM **Llama 3.2** on mental health dataset from the Hugging Face\n",
    "\n",
    "Source:\n",
    "\n",
    "https://www.llama.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Saving model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the dataset\n",
    "\n",
    "NOTE: REPLACE DATASET BELOW WITH DATASET ON PARIS, TEXAS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"hf://datasets/Amod/mental_health_counseling_conversations/combined_dataset.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of words in each context:\n",
    "data['Context_length'] = data['Context'].apply(len)\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(data['Context_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for less than 1500 words:\n",
    "filtered_data = data[data['Context_length'] <= 1500]\n",
    "\n",
    "ln_Context = filtered_data['Context'].apply(len)\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(ln_Context, bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of words in each response:\n",
    "ln_Response = filtered_data['Response'].apply(len)\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(ln_Response, bins=50, kde=True, color='teal')\n",
    "plt.title('Distribution of Response Lengths')\n",
    "plt.xlabel('Length of Response')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for less than 4000 words:\n",
    "filtered_data = filtered_data[ln_Response <= 4000]\n",
    "\n",
    "ln_Response = filtered_data['Response'].apply(len)\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(ln_Response, bins=50, kde=True, color='teal')\n",
    "plt.title('Distribution of Response Lengths')\n",
    "plt.xlabel('Length of Response')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model\n",
    "\n",
    "We are going to use Llama 3.2 with only 1 billion parameters.\n",
    "\n",
    "(You can use the 3, 11 or 90 billion version as well.)\n",
    "\n",
    "- Max Sequence Length:\n",
    "    We used max_seq_length 5020.\n",
    "\n",
    "- Loading Llama 3.2 Model:\n",
    "\n",
    "    - The model and tokenizer are loaded using `FastLanguageModel.from_pretrained` with a specific pre-trained model, \"unsloth/Llama-3.2-1B-bnb-4bitt\". \n",
    "    - This is optimized for 4-bit precision, which reduces memory usage and increases training speed without significantly compromising performance.  \n",
    "    - load_in_4bit=True \n",
    "\n",
    "- Applying PEFT (Parameter-Efficient Fine-Tuning):\n",
    "\n",
    "    - Then we configured model using get_peft_model, which applies LoRA (Low-Rank Adaptation) techniques. \n",
    "    - This approach focuses on fine-tuning only specific layers or parts of the model, rather than the entire network.\n",
    "    - This drastically reduces the computational resources needed.\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "    - r=16\n",
    "    - lora_alpha=16 for target_modules (include key components involved in attention mechanisms like q_proj, k_proj, and v_proj)\n",
    "    - use_rslora=True (activates Rank-Stabilized LoRA())\n",
    "    - use_gradient_checkpointing=\"unsloth\" (memory usage optimized during training)\n",
    "\n",
    "- Verifying Trainable Parameters:\n",
    "    We used `model.print_trainable_parameters()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 5020\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state = 32,\n",
    "    loftq_config = None,\n",
    ")\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for model feed\n",
    "\n",
    "Main points to remember:\n",
    "\n",
    "- Data Prompt Structure:\n",
    "The data_prompt is a formatted string template designed to guide the model in analyzing the provided text. It includes placeholders for the input text (the context) and the model's response. This template specifically prompts the model to identify mental health indicators, making it easier to fine-tune the model for mental health-related tasks.\n",
    "\n",
    "- End-of-Sequence Token:\n",
    "The EOS_TOKEN is retrieved from the tokenizer to signify the end of each text sequence. This token is essential for the model to recognize when a prompt has ended, helping to maintain the structure of the data during training or inference.\n",
    "\n",
    "- Formatting Function:\n",
    "The formatting_prompt used to take a batch of examples and formats them according to the data_prompt. It iterates over the input and output pairs, inserting them into the template and appending the EOS token at the end. The function then returns a dictionary containing the formatted text, ready for model training or evaluation.\n",
    "\n",
    "- Function Output:\n",
    "The function outputs a dictionary where the key is \"text\" and the value is a list of formatted strings. Each string represents a fully prepared prompt for the model, combining the context, response and the structured prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prompt = \"\"\"Analyze the provided text from a mental health perspective. Identify any indicators of emotional distress, coping mechanisms, or psychological well-being. Highlight any potential concerns or positive aspects related to mental health, and provide a brief explanation for each observation.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompt(examples):\n",
    "    inputs       = examples[\"Context\"]\n",
    "    outputs      = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input_, output in zip(inputs, outputs):\n",
    "        text = data_prompt.format(input_, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = Dataset.from_pandas(filtered_data)\n",
    "training_data = training_data.map(formatting_prompt, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training with custom parameters and data\n",
    "\n",
    "In Shell commands (only if necssary):\n",
    "\n",
    "#sudo apt-get update\n",
    "\n",
    "#sudo apt-get install build-essential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training setup to start fine tuning\n",
    "\n",
    "- Trainer Initialization:\n",
    "We are going to initialize SFTTrainer with the model and tokenizer, as well as the training dataset. \n",
    "\n",
    "- Training Arguments:\n",
    "The TrainingArguments class is used to define key hyperparameters for the training process:\n",
    "\n",
    "    - learning_rate=3e-4: Sets the learning rate for the optimizer.\n",
    "    - per_device_train_batch_size=32: Defines the batch size per device, optimizing GPU usage.\n",
    "    - num_train_epochs=20: Specifies the number of training epochs.\n",
    "    - fp16=not is_bfloat16_supported() and bf16=is_bfloat16_supported(): Enable mixed precision training to reduce memory usage, depending on hardware support.\n",
    "    - optim=\"adamw_8bit\": Uses the 8-bit AdamW optimizer for efficient memory usage.\n",
    "    - weight_decay=0.01: Applies weight decay to prevent overfitting.\n",
    "    - output_dir=\"output\": Specifies the directory where the trained model and logs will be saved.\n",
    "\n",
    "- Training Process:\n",
    "\n",
    "    - Finally we called trainer.train() method to start the training process. \n",
    "    - It uses the defined parameters of our fine-tune the model, adjusting weights and learning from the provided dataset. \n",
    "    - The trainer also handles data packing and gradient accumulation, optimizing the training pipeline for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings if necessary:\n",
    "\n",
    "export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "torch.cuda.empty_cache().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=training_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    args=TrainingArguments(\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=40,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=10,\n",
    "        output_dir=\"output\",\n",
    "        seed=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    data_prompt.format(\n",
    "        #instructions\n",
    "        text,\n",
    "        #answer\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 5020, use_cache = True)\n",
    "answer=tokenizer.batch_decode(outputs)\n",
    "answer = answer[0].split(\"### Response:\")[-1]\n",
    "print(\"Answer of the question is:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected response:\n",
    "\n",
    "\"I'm sorry to hear that you are feeling so overwhelmed. It sounds like you are trying to figure out what is going on with you. I would suggest that you see a therapist who specializes in working with people who are struggling with depression. Depression is a common issue that people struggle with. It is important to address the issue of depression in order to improve your quality of life. Depression can lead to other issues such as anxiety, hopelessness, and loss of pleasure in activities. Depression can also lead to thoughts of suicide. If you are thinking of suicide, please call 911 or go to the nearest hospital emergency department. If you are not thinking of suicide, but you are feeling overwhelmed, please call 800-273-8255. This number is free and confidential and you can talk to someone about anything. You can also go to www.suicidepreventionlifeline.org to find a local suicide prevention hotline.<|end_of_text|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push a fine-tuned model and its tokenizer to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"hugging face token key, you can create from your HF account.\"\n",
    "model.push_to_hub(\"ImranzamanML/1B_finetuned_llama3.2\", use_auth_token=os.getenv(\"HF_TOKEN\"))\n",
    "tokenizer.push_to_hub(\"ImranzamanML/1B_finetuned_llama3.2\", use_auth_token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save fine-tuned model and its tokenizer locally on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model/1B_finetuned_llama3.2\")\n",
    "tokenizer.save_pretrained(\"model/1B_finetuned_llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Loading of saved model for usage:\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "\n",
    "model_name = \"model/1B_finetuned_llama3.2\",\n",
    "\n",
    "max_seq_length = 5020,\n",
    "\n",
    "dtype = None,\n",
    "\n",
    "load_in_4bit = True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook:\n",
    "\n",
    "Example:\n",
    "\n",
    "Tutorial: How to Finetune Llama-3 and Use In Ollama\n",
    "\n",
    "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_prompt_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
