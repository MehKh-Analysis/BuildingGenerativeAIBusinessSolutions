{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Benchmarks Explained: Everything on MMLU, HellaSwag, BBH, and Beyond\n",
    "\n",
    "Source:\n",
    "https://www.confident-ai.com/blog/llm-benchmarks-mmlu-hellaswag-and-beyond\n",
    "\n",
    "\n",
    "Tool Used:\n",
    "\n",
    "DeepEval\n",
    "https://github.com/confident-ai/deepeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"./figures/benchmark_1.png\", embed=True, width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different benchmarks assess various aspects of a model’s capabilities, including:\n",
    "\n",
    "- Reasoning and Commonsense: These benchmarks test an LLM’s ability to apply logic and everyday knowledge to solve problems.\n",
    "\n",
    "- Language Understanding and Question Answering (QA): These evaluate a model’s ability to interpret text and answer questions accurately.\n",
    "\n",
    "- Coding: Benchmarks in this category evaluate LLMs on their ability to interpret and generate code.\n",
    "\n",
    "- Conversation and Chatbots: These tests an LLM’s ability to engage in dialogue and provide coherent, relevant responses.\n",
    "\n",
    "- Translation: These assess the model’s ability to accurately translate text from one language to another.\n",
    "\n",
    "- Math: These focus on a model’s ability to solve math problems, from basic arithmetic to more complex areas such as calculus.\n",
    "\n",
    "- Logic: Logic benchmarks evaluate a model’s ability to apply logical reasoning skills, such as inductive and deductive reasoning.\n",
    "\n",
    "- Standardized Tests: SAT, ACT, or other educational assessments are also used to evaluate and benchmark the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"./figures/benchmark_2.png\", embed=True, width = 800, height = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"./figures/benchmark_3.png\", embed=True, width = 1000, height = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of LLM Benchmarks\n",
    "\n",
    "4 most critical domains: Language Understanding, Reasoning, Coding, and Conversation.\n",
    "\n",
    "These benchmarks include:\n",
    "\n",
    "- **TruthfulQA** — Truthfulness\n",
    "- **MMLU** — Language understanding\n",
    "- **HellaSwag** — Commonsense reasoning\n",
    "- **BIG-Bench Hard** — Challenging reasoning tasks\n",
    "- **HumanEval** — Coding challenges\n",
    "- **CodeXGLUE** — Programming tasks\n",
    "- **Chatbot Arena** — Human-ranked ELO-based benchmark\n",
    "- **MT Bench** — Complex conversational ability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom LLM with DeepEval\n",
    "\n",
    "Source: https://docs.confident-ai.com/docs/benchmarks-introduction#benchmarking-your-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "token = open(\"/myapp/local/huggingface-msa8700b.txt\", \"r\").read().strip()\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class Mistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    # This is optional.\n",
    "    def batch_generate(self, promtps: List[str]) -> List[str]:\n",
    "        model = self.load_model()\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer(promtps, return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Mistral7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ollama(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    # This is optional.\n",
    "    def batch_generate(self, promtps: List[str]) -> List[str]:\n",
    "        model = self.load_model()\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer(promtps, return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# ollama = Ollama(model=model, tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "mistral_7b = Mistral7B(model=model, tokenizer=tokenizer)\n",
    "# print(mistral_7b(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Understanding and QA Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TruthfulQA\n",
    "\n",
    "TruthfulQA evaluates models on their ability to provide accurate and truthful answers, which is crucial for combating misinformation and promoting ethical AI usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.benchmarks import TruthfulQA\n",
    "from deepeval.benchmarks.modes import TruthfulQAMode\n",
    "\n",
    "# Define benchmark with specific shots\n",
    "benchmark = TruthfulQA(mode=TruthfulQAMode.MC2)\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "benchmark.evaluate(model=mistral_7b)\n",
    "# benchmark.evaluate(model=ollama)\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MMLU (Massive Multitask Language Understanding)\n",
    "\n",
    "MMLU is aimed at evaluating models based on the knowledge they acquired during pre-training, focusing solely on zero-shot and few-shot settings.\n",
    "\n",
    "MMLU scores an LLM simply based on the proportion of correct answers. The output must be an exact match to be considered correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.benchmarks import MMLU\n",
    "from deepeval.benchmarks.tasks import MMLUTask\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "benchmark = MMLU(\n",
    "    tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],\n",
    "    n_shots=3\n",
    ")\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "benchmark.evaluate(model=mistral_7b)\n",
    "# benchmark.evaluate(model=ollama)\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common-sense and Reasoning Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HellaSwag\n",
    "\n",
    "HellaSwag evaluates the common-sense reasoning capabilities of LLM models through sentence completion. It tests whether LLM models can select the appropriate ending from a set of 4 choices across 10,000 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.benchmarks import HellaSwag\n",
    "from deepeval.benchmarks.tasks import HellaSwagTask\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "benchmark = HellaSwag(\n",
    "    tasks=[HellaSwagTask.TRIMMING_BRANCHES_OR_HEDGES, HellaSwagTask.BATON_TWIRLING],\n",
    "    n_shots=5\n",
    ")\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "benchmark.evaluate(model=mistral_7b)\n",
    "# benchmark.evaluate(model=ollama)\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIG-Bench Hard (Beyond the Imitation Game Benchmark)\n",
    "\n",
    "BIG-Bench Hard (BBH) selects 23 challenging tasks from the original BIG-Bench suite, which consisted of a diverse evaluation set of 204 tasks already beyond the capabilities of language models at the time.\n",
    "\n",
    "Note: \n",
    "\n",
    "The authors of BBH were able to outperform humans on 17 of these tasks with the same exact LLMs using Chain-of-Thought (CoT) prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from deepeval.benchmarks import BigBenchHard\n",
    "from deepeval.benchmarks.tasks import BigBenchHardTask\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "benchmark = BigBenchHard(\n",
    "    tasks=[BigBenchHardTask.BOOLEAN_EXPRESSIONS, BigBenchHardTask.CAUSAL_JUDGEMENT],\n",
    "    n_shots=3,\n",
    "    enable_cot=True\n",
    ")\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "benchmark.evaluate(model=mistral_7b)\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HumanEval\n",
    "\n",
    "HumanEval consists of 164 unique programming tasks designed to evaluate a model’s code generation abilities. \n",
    "\n",
    "These tasks cover a broad spectrum, from algorithms to the comprehension of programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_list(numbers: List[float]) -> float:\n",
    "    return sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.benchmarks import HumanEval\n",
    "\n",
    "# Define benchmark with number of code generations\n",
    "benchmark = HumanEval(n=100)\n",
    "\n",
    "# Replace 'gpt_4' with your own custom model\n",
    "# benchmark.evaluate(model=gpt_4, k=10)\n",
    "benchmark.evaluate(model=mistral_7b, k=10)\n",
    "# benchmark.evaluate(model=ollama, k=10)\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CodeXGLUE\n",
    "\n",
    "CodeXGLUE offers 14 datasets across 10 different tasks to test and compare models directly in various coding scenarios such as code completion, code translation, code summarization, and code search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation and Chatbot Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chatbot Arena\n",
    "\n",
    "The Chatbot Arena is an open platform for ranking language models using over 200K human votes. \n",
    "\n",
    "Users can anonymously quiz and judge pairs of AI models like ChatGPT or Claude without knowing their identities, and votes are counted towards rankings only if the model identities stay hidden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MT Bench\n",
    "\n",
    "MT-bench evaluates chat assistants’ quality by presenting them with a series of multi-turn open-ended questions, utilizing LLMs as judges. \n",
    "\n",
    "This approach tests chat assistants’ ability to handle complex interactions. \n",
    "\n",
    "MT-Bench uses GPT-4 to score on a conversation on a scale of 10, and compute the average score on all turns to get the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
